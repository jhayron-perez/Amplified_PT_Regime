{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eb46b71-652d-4807-afb2-527dab962368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.polynomial import polynomial\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efde26fd-6987-42ae-be9f-676600401758",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f138fa3b-21b1-443a-a866-687c6c07fdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doy_helper(ds):\n",
    "\n",
    "    # Remove leap day (Feb 29)\n",
    "    ds = ds.isel(\n",
    "        time=~((pd.to_datetime(ds.time).day == 29)&(pd.to_datetime(ds.time).month == 2))\n",
    "    )\n",
    "    \n",
    "    # Create day of year (DOY) array for indexing\n",
    "    doy = np.tile(\n",
    "        np.arange(1, 366, 1, dtype='int32'),\n",
    "        len(pd.to_datetime(ds.time).year.unique())\n",
    "    )\n",
    "\n",
    "    # rework dims/coords for climo\n",
    "    return ds.rename({'time':'doy'}).assign_coords({'doy':('doy', doy)})\n",
    "\n",
    "def return_from_doy_helper(dswdoy, dswtime):\n",
    "\n",
    "    # Remove leap day (Feb 29)\n",
    "    dswtime = dswtime.isel(\n",
    "        time=~((pd.to_datetime(dswtime.time).day == 29)&(pd.to_datetime(dswtime.time).month == 2))\n",
    "    )\n",
    "\n",
    "    # save time coord\n",
    "    time_arr = pd.to_datetime(dswtime.time.values)\n",
    "\n",
    "    # reverse to time instead of doy\n",
    "    return dswdoy.assign_coords({'doy':('doy', time_arr)}).rename({'doy':'time'})\n",
    "\n",
    "def get_climatology_smoothed(ds, var, window=60, fileauthor='Jhayron S. Pérez-Carrasquilla'):\n",
    "    \n",
    "    # Subset dataset for the period 1981-2020\n",
    "    ds_clima = ds\n",
    "\n",
    "    # adjust to doy coords/dims\n",
    "    ds_clima = doy_helper(ds_clima)\n",
    "    \n",
    "    # Compute the daily mean for each day of the year\n",
    "    climo = []\n",
    "    for i in range(1, 366):\n",
    "        daily_mean = ds_clima.sel(doy=i)[var].mean('doy')\n",
    "        climo.append(daily_mean)\n",
    "    \n",
    "    # Convert to xarray Dataset with the appropriate dimensions\n",
    "    attrs = ds[var].attrs\n",
    "    attrs['File Author'] = fileauthor\n",
    "    \n",
    "    climo = xr.Dataset({\n",
    "        f'{var}_climo': (['doy', 'lat', 'lon'], np.array(climo)),\n",
    "    }, \n",
    "    coords={\n",
    "        'doy': np.arange(1, 366, 1),\n",
    "        'lat': ds.lat.values,\n",
    "        'lon': ds.lon.values,\n",
    "    },\n",
    "    attrs=attrs\n",
    "    )\n",
    "\n",
    "    # reorder dims\n",
    "    climo = climo.transpose('doy', 'lat', 'lon')\n",
    "\n",
    "    # sanity check\n",
    "    print(climo[f'{var}_climo'].shape)\n",
    "    \n",
    "    # Stack climatology 3 times to handle edges\n",
    "    climo_extended = xr.concat([climo, climo, climo], dim='doy')\n",
    "\n",
    "    # Adjust coordinates after stacking to represent a larger time span\n",
    "    climo_extended['doy'] = np.arange(1, 365 * 3 + 1, 1)\n",
    "\n",
    "    # Apply rolling mean with a 60-day window for smoothing\n",
    "    climo_smoothed = climo_extended.rolling(doy=window, center=True, min_periods=1).mean(skipna=True)\n",
    "\n",
    "    # Extract the middle portion, corresponding to the original 365 days\n",
    "    climo_smoothed = climo_smoothed.isel(doy=slice(365, 365 + 365))\n",
    "\n",
    "    # sanity check\n",
    "    print(climo_smoothed[f'{var}_climo'].shape)\n",
    "    \n",
    "    # Reset 'day_of_year' coordinate to original range\n",
    "    climo_smoothed['doy'] = np.arange(1, 366, 1)\n",
    "\n",
    "    return climo_smoothed\n",
    "\n",
    "def get_anomalies(ds, var, climo):\n",
    "\n",
    "    # deep copy dataset\n",
    "    anom = copy.deepcopy(ds)\n",
    "\n",
    "    # adjust to doy coords/dims\n",
    "    dstmp = doy_helper(ds)\n",
    "    anom = doy_helper(anom)\n",
    "\n",
    "    # compute doy anomalies\n",
    "    for day in range(1, 366):\n",
    "        \n",
    "        anom[var][{'doy':(dstmp.doy == day)}] = (dstmp[var].sel(doy=day) - climo[f'{var}_climo'].sel(doy=day))\n",
    "\n",
    "    # back to dataset\n",
    "    anom = anom.rename({var:f'{var}_anom'})\n",
    "\n",
    "    # add original time dim\n",
    "    return return_from_doy_helper(anom, ds)\n",
    "\n",
    "def fourierfilter(da, cutoff_period=10):\n",
    "    \n",
    "    # Compute the Fourier transform along the time axis\n",
    "    fft_data = np.fft.fft(da, axis=0)\n",
    "    \n",
    "    # Get the frequencies corresponding to the FFT components\n",
    "    freqs = np.fft.fftfreq(da.shape[0], d=1)\n",
    "    # d=1 assumes daily data; adjust if different\n",
    "    \n",
    "    # Compute the corresponding periods (in days)\n",
    "    periods = np.abs(1 / freqs)\n",
    "    \n",
    "    # Define the cutoff period for low-pass filter (10 days)\n",
    "    cutoff_period = cutoff_period\n",
    "    \n",
    "    # Create a mask to filter out high-frequency components (shorter than 10 days)\n",
    "    high_pass_mask = periods < cutoff_period\n",
    "    \n",
    "    # Apply the mask to the FFT data (set high-frequency components to zero)\n",
    "    fft_data_filtered = fft_data.copy()\n",
    "    fft_data_filtered[high_pass_mask, :, :] = 0\n",
    "    \n",
    "    # Perform the inverse FFT to get the filtered data back in the time domain\n",
    "    filtered_data = np.fft.ifft(fft_data_filtered, axis=0).real\n",
    "    \n",
    "    # Create a new xarray DataArray to store the filtered data\n",
    "    filtered_anom = xr.DataArray(\n",
    "        filtered_data,\n",
    "        dims=da.dims,\n",
    "        coords=da.coords,\n",
    "        attrs=da.attrs\n",
    "    )\n",
    "    return filtered_anom\n",
    "\n",
    "# Get average time series of the region\n",
    "def get_weighted_area_average(da):\n",
    "    \n",
    "    # Compute the cosine of the latitudes (in radians) for weighting\n",
    "    # For a rectangular grid the cosine of the latitude is proportional to the grid cell area\n",
    "    weights = np.cos(np.deg2rad(da.lat))\n",
    "    weights.name = \"weights\"\n",
    "\n",
    "    # Apply the weights and compute the mean across lat/lon\n",
    "    da_weighted = da.weighted(weights)\n",
    "    weighted_mean = da_weighted.mean(dim=['lat', 'lon'])\n",
    "    \n",
    "    return weighted_mean\n",
    "\n",
    "\n",
    "def create_doy_dummy(num_yr=84):\n",
    "    \"\"\"Creates dummy array for indexing the non-leap year based doy time series\"\"\"\n",
    "    days_per_year = 365\n",
    "    years = num_yr\n",
    "    day_indices = np.arange(\n",
    "        0, \n",
    "        years * days_per_year, \n",
    "        days_per_year,\n",
    "        dtype=int\n",
    "    ).reshape(-1, 1) + np.arange(days_per_year)\n",
    "    return day_indices\n",
    "\n",
    "\n",
    "def get_climatology_std_smoothed(ds, var, window=60, fileauthor='Jhayron S. Pérez-Carrasquilla'):\n",
    "\n",
    "    # Get the day of year (DOY)\n",
    "    doy_tmp = create_doy_dummy(len(np.unique(pd.to_datetime(ds.time).year)))\n",
    "    \n",
    "    # Compute the daily standard deviation for each day of the year\n",
    "    climo = []\n",
    "    for i in range(0, 365):\n",
    "        # grab indices for doy\n",
    "        doy_indx = doy_tmp[:, i]\n",
    "        \n",
    "        # ensure time is first axis with transpose\n",
    "        array_temp = ds[var].transpose('time', 'lat', 'lon')[doy_indx]\n",
    "\n",
    "        # compute std for doy\n",
    "        std = np.nanstd(array_temp, axis=0)\n",
    "        # make nan where 0\n",
    "        std[std == 0] = np.nan\n",
    "        climo.append(std)\n",
    "    \n",
    "    # Convert to xarray Dataset with the appropriate dimensions\n",
    "    attrs = ds[var].attrs\n",
    "    attrs['File Author'] = fileauthor\n",
    "    \n",
    "    climo = xr.Dataset({\n",
    "        f'{var}_climo_std': (['doy', 'lat', 'lon'], np.array(climo)),\n",
    "    }, \n",
    "    coords={\n",
    "        'doy': np.arange(1, 366, 1),\n",
    "        'lat': ds.lat.values,\n",
    "        'lon': ds.lon.values,\n",
    "    },\n",
    "    attrs=attrs)\n",
    "\n",
    "    climo = climo.transpose('doy', 'lat', 'lon')\n",
    "    \n",
    "    # Stack climatology 3 times to handle edges\n",
    "    climo_extended = xr.concat([climo, climo, climo], dim='doy')\n",
    "\n",
    "    # Adjust coordinates after stacking to represent a larger time span\n",
    "    climo_extended['doy'] = np.arange(1, 365 * 3 + 1, 1)\n",
    "\n",
    "    # Apply rolling mean with a 60-day window for smoothing\n",
    "    climo_smoothed = climo_extended.rolling(doy=window, center=True, min_periods=1).mean(skipna=True)\n",
    "\n",
    "    # Extract the middle portion, corresponding to the original 365 days\n",
    "    climo_smoothed = climo_smoothed.isel(doy=slice(365, 365 + 365))\n",
    "\n",
    "    # Reset 'day_of_year' coordinate to original range\n",
    "    climo_smoothed['doy'] = np.arange(1, 366, 1)\n",
    "\n",
    "    return climo_smoothed\n",
    "\n",
    "# this func is not used (below)\n",
    "def standardize_anomalies(anom, var, climo_std):\n",
    "\n",
    "    # ensure time is first dim/axis\n",
    "    anom = anom.transpose('time', 'lat', 'lon')\n",
    "    std_anom = copy.deepcopy(anom)\n",
    "    \n",
    "    # Get the day of year (DOY)\n",
    "    doy_tmp = create_doy_dummy(len(np.unique(pd.to_datetime(anom.time).year)))\n",
    "\n",
    "    for i in range(0, 365):\n",
    "        \n",
    "        doy_indx = doy_tmp[:, i]\n",
    "\n",
    "        std_anom[var][doy_indx] = (\n",
    "            anom[var][doy_indx] / climo_std[f'{var}_climo_std'].sel(doy=i + 1)\n",
    "        )\n",
    "    return std_anom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27e8c30-5df1-41d9-9bb4-8bf02f90fec3",
   "metadata": {},
   "source": [
    "# Compute anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eedac8d0-17f7-42e6-a708-b6f956178ff4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(365, 61, 151)\n",
      "(365, 61, 151)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/jhayron/tmp/ipykernel_58706/1441930984.py:115: RuntimeWarning: divide by zero encountered in divide\n",
      "  periods = np.abs(1 / freqs)\n"
     ]
    }
   ],
   "source": [
    "path_origins = '/glade/derecho/scratch/jhayron/Data4WRsClimateChange/ProcessedDataReanalyses/'\n",
    "\n",
    "for name_reanalysis in ['ERA5']:\n",
    "    dataset_raw = xr.open_dataset(\n",
    "        f'{path_origins}Z500_{name_reanalysis}.nc'\n",
    "    )\n",
    "    dataset_raw = dataset_raw.sel(time=slice('1979-01-01','2023-12-31'))\n",
    "    lat0=20; lat1=80; lon0=180; lon1=330\n",
    "    dataset_region = dataset_raw.where(\n",
    "        (\n",
    "            dataset_raw.lat>=lat0)&(\n",
    "                dataset_raw.lat<=lat1)&(\n",
    "                dataset_raw.lon>=lon0)&(\n",
    "                dataset_raw.lon<=lon1),\n",
    "        drop=True\n",
    "    )\n",
    "    dataset_region.Z.data = dataset_region.Z.data / 9.82 ### Divide by gravity to convert from m2/s2 to m\n",
    "    \n",
    "    clima = get_climatology_smoothed(dataset_region, 'Z')\n",
    "    anoms = get_anomalies(dataset_region, 'Z', clima)\n",
    "    \n",
    "    anoms_filtered = fourierfilter(anoms.Z_anom)\n",
    "    anoms_filtered = anoms_filtered.to_dataset(name='Z_anom')\n",
    "    anoms_smooth = copy.deepcopy(anoms_filtered).rolling(time=60, center=True, min_periods=1).mean(skipna=True)\n",
    "    mean_series = get_weighted_area_average(anoms_smooth.Z_anom)\n",
    "    full_curve = copy.deepcopy(mean_series)\n",
    "    full_curve.data = np.zeros(len(full_curve))\n",
    "    npoly=3\n",
    "    \n",
    "    doy_tmp = create_doy_dummy(len(np.unique(pd.to_datetime(mean_series.time).year)))\n",
    "    \n",
    "    for i in range(0, 365): #Iterate through every day of the year\n",
    "    \n",
    "        doy_indx = doy_tmp[:, i]\n",
    "        \n",
    "        # fit a polynomial for the trend of each DOY\n",
    "        params_curve = polynomial.polyfit(\n",
    "            np.arange(0, mean_series[doy_indx].shape[0]), \n",
    "            mean_series[doy_indx], \n",
    "            npoly\n",
    "        )\n",
    "        curve = polynomial.polyval(\n",
    "            np.arange(0, mean_series[doy_indx].shape[0]), \n",
    "            params_curve, \n",
    "            tensor=True\n",
    "        )\n",
    "        \n",
    "        ## Center curve in zero\n",
    "        full_curve.loc[{'time': mean_series[doy_indx].time}] = curve\n",
    "    \n",
    "    anoms_detrended = anoms_filtered.Z_anom - full_curve.data[:, np.newaxis, np.newaxis]\n",
    "    anoms_detrended = anoms_detrended.to_dataset()\n",
    "    clima_std = get_climatology_std_smoothed(anoms_detrended, 'Z_anom')\n",
    "    clima_std_average_region_series = get_weighted_area_average(clima_std)\n",
    "    \n",
    "    anoms_standardized = copy.deepcopy(anoms_detrended).Z_anom\n",
    "    anoms_standardized.data = np.zeros_like(anoms_standardized.data)\n",
    "    \n",
    "    doy_tmp = create_doy_dummy(len(np.unique(pd.to_datetime(mean_series.time).year)))\n",
    "    \n",
    "    for i in range(0, 365): #Iterate through every day of the year\n",
    "    \n",
    "        doy_indx = doy_tmp[:, i]\n",
    "    \n",
    "        # grab doy std (climo)\n",
    "        std_temp = clima_std_average_region_series.Z_anom_climo_std[i].data\n",
    "        \n",
    "        # standardize the detrended anoms by std climo\n",
    "        standardized_temp = anoms_detrended.Z_anom[doy_indx].data / std_temp\n",
    "        \n",
    "        anoms_standardized.loc[{'time': anoms_detrended.Z_anom[doy_indx].time}] = standardized_temp\n",
    "    \n",
    "    anoms_standardized = anoms_standardized.to_dataset()\n",
    "    anoms_standardized.to_netcdf(f'{path_origins}Z500Anoms_{name_reanalysis}_19792023.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b5abb4f-086a-470b-85a0-73328d48f058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(365, 61, 151)\n",
      "(365, 61, 151)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/jhayron/tmp/ipykernel_58706/1441930984.py:115: RuntimeWarning: divide by zero encountered in divide\n",
      "  periods = np.abs(1 / freqs)\n"
     ]
    }
   ],
   "source": [
    "path_origins = '/glade/derecho/scratch/jhayron/Data4WRsClimateChange/ProcessedDataReanalyses/'\n",
    "\n",
    "for name_reanalysis in ['ERA5']:\n",
    "    dataset_raw = xr.open_dataset(\n",
    "        f'{path_origins}Z500_{name_reanalysis}.nc'\n",
    "    )\n",
    "    lat0=20; lat1=80; lon0=180; lon1=330\n",
    "    dataset_raw = dataset_raw.sel(time=slice('1940-01-01','1978-12-31'))\n",
    "\n",
    "    \n",
    "    dataset_region = dataset_raw.where(\n",
    "        (\n",
    "            dataset_raw.lat>=lat0)&(\n",
    "                dataset_raw.lat<=lat1)&(\n",
    "                dataset_raw.lon>=lon0)&(\n",
    "                dataset_raw.lon<=lon1),\n",
    "        drop=True\n",
    "    )\n",
    "    \n",
    "    \n",
    "    dataset_region.Z.data = dataset_region.Z.data / 9.82 ### Divide by gravity to convert from m2/s2 to m\n",
    "    \n",
    "    clima = get_climatology_smoothed(dataset_region, 'Z')\n",
    "    anoms = get_anomalies(dataset_region, 'Z', clima)\n",
    "    \n",
    "    anoms_filtered = fourierfilter(anoms.Z_anom)\n",
    "    anoms_filtered = anoms_filtered.to_dataset(name='Z_anom')\n",
    "    anoms_smooth = copy.deepcopy(anoms_filtered).rolling(time=60, center=True, min_periods=1).mean(skipna=True)\n",
    "    mean_series = get_weighted_area_average(anoms_smooth.Z_anom)\n",
    "    full_curve = copy.deepcopy(mean_series)\n",
    "    full_curve.data = np.zeros(len(full_curve))\n",
    "    npoly=3\n",
    "    \n",
    "    doy_tmp = create_doy_dummy(len(np.unique(pd.to_datetime(mean_series.time).year)))\n",
    "    \n",
    "    for i in range(0, 365): #Iterate through every day of the year\n",
    "    \n",
    "        doy_indx = doy_tmp[:, i]\n",
    "        \n",
    "        # fit a polynomial for the trend of each DOY\n",
    "        params_curve = polynomial.polyfit(\n",
    "            np.arange(0, mean_series[doy_indx].shape[0]), \n",
    "            mean_series[doy_indx], \n",
    "            npoly\n",
    "        )\n",
    "        curve = polynomial.polyval(\n",
    "            np.arange(0, mean_series[doy_indx].shape[0]), \n",
    "            params_curve, \n",
    "            tensor=True\n",
    "        )\n",
    "        \n",
    "        ## Center curve in zero\n",
    "        full_curve.loc[{'time': mean_series[doy_indx].time}] = curve\n",
    "    \n",
    "    anoms_detrended = anoms_filtered.Z_anom - full_curve.data[:, np.newaxis, np.newaxis]\n",
    "    anoms_detrended = anoms_detrended.to_dataset()\n",
    "    clima_std = get_climatology_std_smoothed(anoms_detrended, 'Z_anom')\n",
    "    clima_std_average_region_series = get_weighted_area_average(clima_std)\n",
    "    \n",
    "    anoms_standardized = copy.deepcopy(anoms_detrended).Z_anom\n",
    "    anoms_standardized.data = np.zeros_like(anoms_standardized.data)\n",
    "    \n",
    "    doy_tmp = create_doy_dummy(len(np.unique(pd.to_datetime(mean_series.time).year)))\n",
    "    \n",
    "    for i in range(0, 365): #Iterate through every day of the year\n",
    "    \n",
    "        doy_indx = doy_tmp[:, i]\n",
    "    \n",
    "        # grab doy std (climo)\n",
    "        std_temp = clima_std_average_region_series.Z_anom_climo_std[i].data\n",
    "        \n",
    "        # standardize the detrended anoms by std climo\n",
    "        standardized_temp = anoms_detrended.Z_anom[doy_indx].data / std_temp\n",
    "        \n",
    "        anoms_standardized.loc[{'time': anoms_detrended.Z_anom[doy_indx].time}] = standardized_temp\n",
    "    \n",
    "    anoms_standardized = anoms_standardized.to_dataset()\n",
    "    anoms_standardized.to_netcdf(f'{path_origins}Z500Anoms_{name_reanalysis}_19401978.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a37e8b-d815-4869-af00-a5ca50c85617",
   "metadata": {},
   "source": [
    "# compute PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f32c6d56-20af-4b21-bac0-eb25b7743f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7621c2b1-f7c8-40ae-869c-4a0990c4766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pcs(dataarray):\n",
    "    dataflattened = dataarray.Z_anom.stack(flat=('lat','lon')).transpose('time','flat')\n",
    "    pca_obj = PCA(12, whiten=True)\n",
    "    pca_obj = pca_obj.fit(dataflattened)\n",
    "    datatransformed = pca_obj.transform(dataflattened)\n",
    "\n",
    "    variance_explained = np.sum(pca_obj.explained_variance_ratio_) * 100\n",
    "    return datatransformed, variance_explained, pca_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d05e8744-8c0b-409f-b33c-253d305e87b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/jhayron/Data4WRsClimateChange/PCs_Z500//PCs_ERA5_19401978.csv\n",
      "/glade/derecho/scratch/jhayron/Data4WRsClimateChange/PCs_Z500//PCs_ERA5_19792023.csv\n"
     ]
    }
   ],
   "source": [
    "path_pcs = '/glade/derecho/scratch/jhayron/Data4WRsClimateChange/PCs_Z500/'\n",
    "path_files = '/glade/derecho/scratch/jhayron/Data4WRsClimateChange/ProcessedDataReanalyses/'\n",
    "reanalysis = 'ERA5'\n",
    "for timeperiod in ['19401978','19792023']:\n",
    "    anoms = xr.open_dataset(f'{path_files}Z500Anoms_{reanalysis}_{timeperiod}.nc')\n",
    "    pcs, variance_explained_temp, pca_obj = compute_pcs(anoms)\n",
    "\n",
    "    filename = f'{path_pcs}PCs_{reanalysis}_{timeperiod}.pca_obj'\n",
    "    joblib.dump(pca_obj, filename)\n",
    "\n",
    "    pcs = pd.DataFrame(pcs,index = anoms.time)\n",
    "    pcs.to_csv(f'{path_pcs}/PCs_{reanalysis}_{timeperiod}.csv')\n",
    "    print(f'{path_pcs}/PCs_{reanalysis}_{timeperiod}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc2deea-3047-488c-b576-8305cc69593e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_wr]",
   "language": "python",
   "name": "conda-env-pytorch_wr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
