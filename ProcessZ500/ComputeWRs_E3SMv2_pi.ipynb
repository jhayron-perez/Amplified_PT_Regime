{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ccd112f-b570-4953-9325-1a5173c413bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from itertools import product\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.path as mpath\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.io.shapereader as shpreader\n",
    "import cartopy.feature as cf\n",
    "import shapely.geometry as sgeom\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "import pickle\n",
    "import copy\n",
    "from shapely import geometry\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96d29fd-9065-4ecb-a6ad-9e2d0addf1f3",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8ca8a9b-07ef-4539-862c-9c492d791b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_region(data_array, regioncoords):\n",
    "    \"\"\"\n",
    "    Extract a region from a DataArray with longitudes ranging from 0 to 360.\n",
    "    Rearrange the data to have continuous longitudes even if the region wraps around 0 or 360 degrees.\n",
    "    \n",
    "    Parameters:\n",
    "    data_array (xr.DataArray): Input DataArray with coordinates 'lon' (0 to 360) and 'lat'.\n",
    "    min_lon (float): Minimum longitude of the region.\n",
    "    max_lon (float): Maximum longitude of the region.\n",
    "    min_lat (float): Minimum latitude of the region.\n",
    "    max_lat (float): Maximum latitude of the region.\n",
    "    \n",
    "    Returns:\n",
    "    xr.DataArray: Extracted region with continuous coordinates.\n",
    "    \"\"\"\n",
    "    min_lon, max_lon, min_lat, max_lat = regioncoords\n",
    "\n",
    "    # Handle the case where the region crosses the prime meridian or the International Date Line\n",
    "    if min_lon > max_lon:\n",
    "        # Split the region into two parts: one on the left of 0Â° and one on the right\n",
    "        region1 = data_array.sel(\n",
    "            lon=slice(min_lon, 360), \n",
    "            lat=slice(min_lat, max_lat)\n",
    "        )\n",
    "        region2 = data_array.sel(\n",
    "            lon=slice(0, max_lon), \n",
    "            lat=slice(min_lat, max_lat)\n",
    "        )\n",
    "\n",
    "        # Combine the two parts along the longitude axis\n",
    "        combined_region = xr.concat([region1, region2], dim='lon')\n",
    "\n",
    "    else:\n",
    "        # Directly slice the region\n",
    "        combined_region = data_array.sel(\n",
    "            lon=slice(min_lon, max_lon), \n",
    "            lat=slice(min_lat, max_lat)\n",
    "        )\n",
    "\n",
    "    # # Sort the longitudes to ensure they are in the correct order\n",
    "    # combined_region = combined_region.sortby('lon')\n",
    "\n",
    "    return combined_region\n",
    "    \n",
    "def get_average_fields_for_centroids(dataarray,labels):\n",
    "    dataarray = dataarray.drop_duplicates('time',keep='first')\n",
    "    labels = labels[~labels.index.duplicated(keep='first')]\n",
    "    \n",
    "    wrs = np.unique(labels['WR'])\n",
    "    avgs = []\n",
    "    for wr in wrs:\n",
    "        df_wr = labels[labels['WR']==wr]\n",
    "        arr_selection = dataarray.sel(time=df_wr.index)\n",
    "        averagefield = arr_selection.mean('time')\n",
    "        avgs.append(averagefield)\n",
    "    return xr.concat(avgs,dim='WR')\n",
    "\n",
    "import math\n",
    "\n",
    "def plot_multiple_maps(da,freqs_labels,regioncoords,names = None, path_save=None, n_cols=2):\n",
    "    \"\"\"\n",
    "    Plot multiple maps from a list of data arrays with a fixed number of columns and dynamic rows.\n",
    "    \n",
    "    Parameters:\n",
    "    - da_list: list of xarray.DataArray objects to plot.\n",
    "    - n_cols: Number of columns for the subplot grid (default is 2).\n",
    "    \"\"\"\n",
    "\n",
    "    min_lon, max_lon, min_lat, max_lat = regioncoords\n",
    "    # Convert longitudes from 0-360 to -180-180 if necessary\n",
    "    def convert_lon(lon):\n",
    "        return lon if lon <= 180 else lon - 360\n",
    "    \n",
    "    min_lon_converted = convert_lon(min_lon)\n",
    "    max_lon_converted = convert_lon(max_lon)\n",
    "    \n",
    "    # Number of maps to plot\n",
    "    n_maps = len(da.WR)\n",
    "    \n",
    "    # Determine the number of rows needed\n",
    "    n_rows = math.ceil(n_maps / n_cols)\n",
    "    \n",
    "    # Create a figure with the calculated number of subplots\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(9, 2 * (n_rows)),\n",
    "                             subplot_kw={'projection': ccrs.PlateCarree(central_longitude=-100)})\n",
    "\n",
    "    # If there's only one row, axes will not be a 2D array, so we need to adjust for that\n",
    "    if n_rows == 1:\n",
    "        axes = np.expand_dims(axes, axis=0)\n",
    "    \n",
    "    # Plot each DataArray in the provided list\n",
    "    for i in range(len(da.WR.values)-1):\n",
    "        row = i // n_cols\n",
    "        col = i % n_cols\n",
    "        \n",
    "        ax = axes[row, col]\n",
    "\n",
    "        # Convert longitudes to -180 to 180 range\n",
    "        lon = (da.lon + 180) % 360 - 180\n",
    "    \n",
    "        # Adjust data array to match the longitude range\n",
    "        da_shifted, lon_shifted = xr.broadcast(da, lon)\n",
    "        \n",
    "        # Plot the rectangle to highlight the specified region\n",
    "        rect_style = {'edgecolor': 'black', 'facecolor': 'gray', 'linewidth': 1.5, 'alpha':0.2}\n",
    "        \n",
    "        ax.set_extent([min_lon, max_lon, min_lat, max_lat], crs=ccrs.PlateCarree())\n",
    "\n",
    "        # Add gridlines every 20 degrees\n",
    "        gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                          linewidth=0.2, color='gray', alpha=0.5, linestyle='--')\n",
    "        gl.xlocator = plt.FixedLocator(np.arange(-180, 181, 60))\n",
    "        gl.ylocator = plt.FixedLocator(np.arange(0, 91, 20))\n",
    "        gl.top_labels = False\n",
    "        gl.right_labels = False\n",
    "        gl.xformatter = LongitudeFormatter(zero_direction_label=True)\n",
    "        gl.yformatter = LatitudeFormatter()\n",
    "\n",
    "        tick_fontsize = 10\n",
    "        # Set font size for tick labels\n",
    "        gl.xlabel_style = {'size': tick_fontsize}\n",
    "        gl.ylabel_style = {'size': tick_fontsize}\n",
    "\n",
    "        # Plot the data using pcolormesh\n",
    "        mini=-2\n",
    "        maxi=2\n",
    "        intervals = 21\n",
    "        bounds=np.linspace(mini,maxi,intervals)\n",
    "        mesh = ax.contourf(lon_shifted.sel(WR=i).lon, da.lat, da_shifted.sel(WR=i).values, levels=bounds, vmin=mini, vmax=maxi,\n",
    "                                 cmap='bwr', transform=ccrs.PlateCarree(),extend='both')\n",
    "\n",
    "        # Add coastlines for context\n",
    "        ax.coastlines()\n",
    "\n",
    "        # Set title for each subplot\n",
    "        if names:\n",
    "            ax.set_title(f'{names[i]} - Freq.: {np.round(freqs_labels[i],2)}%',fontsize=11)\n",
    "        else:\n",
    "            ax.set_title(f'Cluster {i+1} - Freq.: {np.round(freqs_labels[i],2)}%',fontsize=11)\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(i + 1, n_rows * n_cols):\n",
    "        fig.delaxes(axes[j // n_cols, j % n_cols])\n",
    "\n",
    "    # # Adjust layout to prevent overlapping\n",
    "    plt.tight_layout(w_pad=0.1)\n",
    "    # Alternatively, adjust spacing between plots using subplots_adjust\n",
    "    # fig.subplots_adjust(hspace=-0.7, wspace=0.3)  # Adjust these parameters as needed\n",
    "\n",
    "\n",
    "    cax = fig.add_axes([0.55, 0.22, 0.4, 0.03])  # Example position: horizontal, below the main plot\n",
    "    \n",
    "    # Add a horizontal colorbar\n",
    "    cbar = fig.colorbar(mesh, cax=cax, orientation='horizontal')\n",
    "    cbar.set_label(r'Z Anomaly ($\\sigma$)')\n",
    "\n",
    "    # main_title = f\"Region: {min_lon,max_lon,min_lat,max_lat}\"\n",
    "    # Add the overall title for the figure\n",
    "    fig.suptitle('CESM2_pi - Weather Regimes', fontsize=14, y=1.04,ha='center')\n",
    "\n",
    "    if path_save==False:\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "        plt.close('all')\n",
    "    else:\n",
    "        plt.savefig(path_save, bbox_inches='tight',dpi=200)\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26cbbe8d-ebf9-4574-9301-2d99fd8ee17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pcs(dataarray):\n",
    "    dataflattened = dataarray.stack(flat=('lat','lon')).transpose('time','flat')\n",
    "    pca_obj = PCA(12, whiten=True)\n",
    "    pca_obj = pca_obj.fit(dataflattened)\n",
    "    datatransformed = pca_obj.transform(dataflattened)\n",
    "\n",
    "    variance_explained = np.sum(pca_obj.explained_variance_ratio_) * 100\n",
    "    return datatransformed, variance_explained\n",
    "    \n",
    "def compute_wrs_seeded(dataarray,n=5):\n",
    "    dataflattened = dataarray.stack(flat=('lat','lon')).transpose('time','flat')\n",
    "    \n",
    "    pca_obj = PCA(12, whiten=True)\n",
    "    pca_obj = pca_obj.fit(dataflattened)\n",
    "    datatransformed = pca_obj.transform(dataflattened)\n",
    "\n",
    "    variance_explained = np.sum(pca_obj.explained_variance_ratio_) * 100\n",
    "    \n",
    "    # train kmeans # transfer learning\n",
    "    k_means = KMeans(n_clusters=n,\n",
    "                     # init='k-means++',\n",
    "                     init=era5_clusters_centers,\n",
    "                     n_init=1,\n",
    "                     max_iter=300, #(30/75) * 300\n",
    "                     tol=0.0001,\n",
    "                     verbose=0,\n",
    "                     random_state=42)\n",
    "    k_means.fit(datatransformed)\n",
    "    clusters_centers = k_means.cluster_centers_\n",
    "    labels = k_means.labels_\n",
    "    distances = euclidean_distances(clusters_centers, datatransformed)\n",
    "\n",
    "    return clusters_centers, labels, distances, variance_explained, datatransformed, k_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d911444-4d50-4e97-82cc-0654a90fbd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_EOFs_from_PCs(PCs,da_region):\n",
    "    # Assuming the original data matrix is called 'X' and has dimensions [N, M]\n",
    "    # 'PC' is the principal components matrix and has dimensions [N, K]\n",
    "    # We want to recover the 'EOF' matrix, which will have dimensions [M, K]\n",
    "    \n",
    "    # Calculate the pseudo-inverse of PC\n",
    "    if isinstance(PCs, np.ndarray):\n",
    "        PC_pseudo_inv = np.linalg.pinv(PCs)\n",
    "    else:\n",
    "        PC_pseudo_inv = np.linalg.pinv(PCs.values)\n",
    "    \n",
    "    # Compute the EOFs using the equation above\n",
    "    EOF = np.dot(PC_pseudo_inv, da_region.stack(flat=('lat','lon')).transpose('time','flat'))\n",
    "    \n",
    "    # EOF will now have dimensions [K, M]. If you want [M, K], transpose the result.\n",
    "    EOF = EOF.T\n",
    "    \n",
    "    nlat, nlon = len(da_region['lat']), len(da_region['lon'])  # Retrieve the number of latitudes and longitudes\n",
    "    EOF_reshaped = EOF.reshape(nlat, nlon, EOF.shape[-1])  # Shape to [lat, lon, K]\n",
    "    \n",
    "    # Create an xarray.DataArray with appropriate coordinates and dimensions\n",
    "    EOF_xr = xr.DataArray(\n",
    "        EOF_reshaped,\n",
    "        dims=['lat', 'lon', 'mode'],  # Specify dimensions: latitude, longitude, and mode (PC index)\n",
    "        coords={\n",
    "            'lat': da_region.coords['lat'],\n",
    "            'lon': da_region.coords['lon'],\n",
    "            'mode': np.arange(EOF.shape[-1])  # Create a mode coordinate [1, 2, ..., K]\n",
    "        },\n",
    "        name='EOFs'\n",
    "    )\n",
    "    return EOF_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "742c9575-76e7-4be4-91e5-3a6d1e11ae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "def reorder_model_eofs(obs_eofs, model_eofs):\n",
    "    \"\"\"\n",
    "    Reorders the model EOFs to match the observation EOFs based on spatial correlation.\n",
    "\n",
    "    Parameters:\n",
    "    - obs_eofs: xarray.DataArray containing observation EOFs with dimensions [lat, lon, mode]\n",
    "    - model_eofs: xarray.DataArray containing model EOFs with dimensions [lat, lon, mode]\n",
    "\n",
    "    Returns:\n",
    "    - reordered_indices: List containing the index of model EOFs that best match each observation EOF.\n",
    "    \"\"\"\n",
    "    # Initialize a correlation matrix to store the spatial correlation between each pair of EOFs\n",
    "    n_obs_modes = obs_eofs.shape[-1]  # Number of EOF modes in the observation\n",
    "    n_model_modes = model_eofs.shape[-1]  # Number of EOF modes in the model\n",
    "\n",
    "    # Flatten lat/lon dimensions for correlation calculation\n",
    "    obs_flat = obs_eofs.stack(spatial=('lat', 'lon'))  # Shape: [spatial, mode]\n",
    "    model_flat = model_eofs.stack(spatial=('lat', 'lon'))  # Shape: [spatial, mode]\n",
    "\n",
    "    # Compute spatial correlation between each observation EOF and each model EOF\n",
    "    correlation_matrix = np.zeros((n_obs_modes, n_model_modes))\n",
    "\n",
    "    for i in range(n_obs_modes):\n",
    "        # print(i)\n",
    "        for j in range(n_model_modes):\n",
    "            obs_eof = obs_flat.sel(mode=i)\n",
    "            model_eof = model_flat.sel(mode=j)\n",
    "\n",
    "            # Calculate Pearson correlation coefficient\n",
    "            correlation = np.corrcoef(obs_eof, model_eof)[0,1]\n",
    "            # correlation = xr.corr(obs_eof, model_eof, dim='spatial')\n",
    "            correlation_matrix[i, j] = correlation\n",
    "\n",
    "    # For each observation EOF, find the best matches among model EOFs\n",
    "    reordered_indices = []\n",
    "    signs = []  # Store +1 or -1 to indicate sign of correlation\n",
    "    matched_model_eofs = set()\n",
    "\n",
    "    for i in range(n_obs_modes):\n",
    "        # Get the indices of model EOFs sorted by absolute correlation (highest to lowest)\n",
    "        sorted_model_indices = np.argsort(-np.abs(correlation_matrix[i, :]))\n",
    "\n",
    "        # Find the best available match that hasn't been used yet\n",
    "        for best_match in sorted_model_indices:\n",
    "            if best_match not in matched_model_eofs:\n",
    "                reordered_indices.append(best_match)\n",
    "                matched_model_eofs.add(best_match)\n",
    "                \n",
    "                # Determine the sign of correlation (+1 or -1) based on the actual correlation value\n",
    "                sign = 1 if correlation_matrix[i, best_match] >= 0 else -1\n",
    "                signs.append(sign)\n",
    "                break\n",
    "\n",
    "    return reordered_indices, signs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9628c4-02d1-47c3-b061-2e6e19d1bc64",
   "metadata": {},
   "source": [
    "# Compute WRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee23d8d6-8146-460d-983b-e07a60efaaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/jhayron/conda-envs/cnn_wr/lib/python3.9/site-packages/numpy/lib/function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/glade/work/jhayron/conda-envs/cnn_wr/lib/python3.9/site-packages/numpy/lib/function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Load the saved object\n",
    "with open('kmeans_models/k_means_model_era5.pkl', 'rb') as f:\n",
    "    k_means = pickle.load(f)\n",
    "path_files = '/glade/derecho/scratch/jhayron/Data4WRsClimateChange/ProcessedDataReanalyses/'\n",
    "path_pcs = '/glade/derecho/scratch/jhayron/Data4WRsClimateChange/PCs_Z500/'\n",
    "\n",
    "reanalysis = 'ERA5'\n",
    "anoms_era5 = xr.open_dataset(f'{path_files}Z500Anoms_{reanalysis}.nc')\n",
    "region = [180, 330, 20, 80]\n",
    "data_region_era5 = extract_region(anoms_era5, region)\n",
    "pcs_era5 = pd.read_csv(f'{path_pcs}PCs_{reanalysis}.csv',\n",
    "                       index_col=0,parse_dates=True, names=np.arange(0,12),skiprows=1)\n",
    "\n",
    "clusters_centers = k_means.cluster_centers_\n",
    "labels = k_means.labels_\n",
    "cluster_centers = np.vstack([clusters_centers,np.zeros(12)])\n",
    "distances = euclidean_distances(cluster_centers, pcs_era5.values)\n",
    "labels_era5 = distances.argmin(axis=0)\n",
    "\n",
    "df_labels = pd.DataFrame(labels_era5,index=pcs_era5.index)\n",
    "df_labels.columns=['WR']\n",
    "df_labels['distances'] = distances.min(axis=0)\n",
    "corrs = np.array([np.corrcoef(pcs_era5.values[i],cluster_centers[df_labels['WR'].iloc[i]])[0,1] for i in range(len(df_labels))])\n",
    "df_labels['corr'] = corrs\n",
    "# df_labels.loc[df_labels['corr']<=0.25,'WR']=np.unique(df_labels['WR'])[-1]\n",
    "\n",
    "labels_era5 = df_labels['WR'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28af85bb-418a-4450-8986-b083be276344",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOF_ERA5 = get_EOFs_from_PCs(pcs_era5,data_region_era5.Z_anom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e701c4d9-ef8c-4abf-a04f-cafb5429c06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_anoms = '/glade/derecho/scratch/jhayron/Data4WRsClimateChange/E3SMv2_pi/anoms_standardized.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "864737dc-b76c-4a90-8045-bffe816a2f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder PCs from the model\n",
    "anoms = xr.open_dataset(path_anoms)\n",
    "anoms = anoms.Z_anom.compute()\n",
    "# anoms = anoms\n",
    "region = [180, 330, 20, 80]\n",
    "data_region = extract_region(anoms, region)#.sel(time=slice(None, '2100-12-31'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd44a036-b1df-4948-bd02-ff14e6bbdd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs_model, variance_explained_member = compute_pcs(data_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69ab3f53-c1dc-4aa0-ae28-283a95582e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOF_model = get_EOFs_from_PCs(pcs_model,data_region)\n",
    "EOF_ERA5_modelcoords = EOF_ERA5.sel(lat=EOF_model.lat,lon=EOF_model.lon,method='nearest')\n",
    "indices_pcs_ordered, signs_pcs = reorder_model_eofs(EOF_ERA5_modelcoords, EOF_model)\n",
    "pcs_model = pcs_model[:,indices_pcs_ordered] * signs_pcs\n",
    "pcs_model = pd.DataFrame(pcs_model,index = data_region.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a9ef246-b7fd-4ec7-a723-d8073bc37e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/jhayron/conda-envs/cnn_wr/lib/python3.9/site-packages/numpy/lib/function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/glade/work/jhayron/conda-envs/cnn_wr/lib/python3.9/site-packages/numpy/lib/function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    }
   ],
   "source": [
    "distances_model = euclidean_distances(cluster_centers, pcs_model.values)\n",
    "labels_model = distances_model.argmin(axis=0)\n",
    "df_labels_model = pd.DataFrame(labels_model,index=pcs_model.index,columns = ['WR'])\n",
    "\n",
    "df_labels_model['distances'] = distances_model.min(axis=0)\n",
    "corrs = np.array([np.corrcoef(pcs_model.values[i],cluster_centers[df_labels_model['WR'].iloc[i]])[0,1] for i in range(len(df_labels_model))])\n",
    "df_labels_model['corr'] = corrs\n",
    "# df_labels_model.loc[df_labels_model['corr']<=0.25,'WR']=np.unique(df_labels_model['WR'])[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10a61974-c805-47c4-84bc-466eb07a6951",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_model.to_csv(f'labels_pi/df_labels_pi_e3smv2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d49132a6-7d4c-40af-bcb9-9e80f132e70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgs_model = get_average_fields_for_centroids(anoms,df_labels_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc66bb14-9f1a-4344-b5a8-62751cd3c5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_model = []\n",
    "for label in np.unique(labels_model):\n",
    "    freqs_model.append(100*len(labels_model[labels_model==label])/len(labels_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b3a77fd-88e2-4439-9530-4e1b8aac9537",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Polar High\", \"Pacific Trough\", \"Pacific Ridge\", \"Alaskan Ridge\", \"Atlantic Ridge\" ,\"No WR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a29a19d3-265e-4d5c-ab55-c42ee84d3054",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiple_maps(avgs_model,freqs_model,region,names=names, \n",
    "                   path_save=f'Figures/Composites_E3SMv2pi.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eee564-8c63-4055-b472-81b49ccf3c27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cnn_wr]",
   "language": "python",
   "name": "conda-env-cnn_wr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
